{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c4df2",
   "metadata": {},
   "source": [
    "# Aula 12 - Programação de Linguagem Natural - 31/10/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68605304",
   "metadata": {},
   "source": [
    "# O que é Programação de Linguagem Natural (PLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e044f81",
   "metadata": {},
   "source": [
    "**PLN (ou NLP - Natural Language Processing)** é uma subárea da Inteligência Artificial que permite aos computadores **entenderem, interpretarem e gerarem linguagem humana** de forma útil.\n",
    "\n",
    "Ela une **linguística computacional**, **estatística**, **aprendizado de máquina** e **ciência de dados** para extrair valor da linguagem humana.\n",
    "\n",
    "**Ex.:**\n",
    "1. **Chatbots (ex. atendimento automático no whatsapp ou sites)**\n",
    "- **Como é feito**: O chatbot utiliza PLN para entender as intenções do usuário. Ele identifica palavras-chave, faz análise sintática e semântica e aciona uma resposta.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização**: quebra a frase em palavras.\n",
    "    - **Reconhecimento de intenções (intent detection)**: identifica o que o usuário quer (ex: “quero pagar minha fatura” → intenção = pagamento).\n",
    "    - **Extração de entidades (NER)**: reconhece dados como datas, nomes, números.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"Quero saber meu saldo.\"*\n",
    "    - O chatbot detecta a intenção \"consultar_saldo\" e busca a informação no sistema.\n",
    "---\n",
    "2. **Corretores ortográficos**\n",
    "\n",
    "- **Como é feito**: O sistema compara as palavras escritas com um dicionário. Se uma palavra não está no vocabulário, o sistema sugere a mais parecida usando algoritmos como distância de Levenshtein.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização**: identifica cada palavra digitada.\n",
    "    - **Verificação de vocabulário**: confere se a palavra existe.\n",
    "    - **Sugerir correção**: calcula quais palavras são mais próximas.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"Recomendo esse filmee.\"*\n",
    "    - Saída: “Você quis dizer: *filme*?”\n",
    "---\n",
    "3. **Análise de sentimentos**\n",
    "\n",
    "- **Como é feito**: O sistema identifica se um texto expressa opinião positiva, negativa ou neutra.\n",
    "- **Etapas usadas**:\n",
    "    - **Remoção de stop words**, stemming.\n",
    "    - **Vetorização** e uso de modelos (estatísticos ou redes neurais).\n",
    "- **Exemplo real**:\n",
    "    - Comentário: *\"O filme foi incrível!\"*\n",
    "    - Saída: sentimento = *positivo*\n",
    "---\n",
    "4. **Tradução automática (Google Tradutor, DeepL)**\n",
    "\n",
    "- **Como é feito**: Utiliza redes neurais treinadas com milhões de pares de frases em dois idiomas.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização e análise gramatical**.\n",
    "    - **Mapeamento semântico**: entender o significado e a função de cada palavra.\n",
    "    - **Geração de texto no idioma de destino**.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"I love programming.\"*\n",
    "    - Tradução: *\"Eu amo programar.\"*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75139f00",
   "metadata": {},
   "source": [
    "# Etapas de Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161ffdc",
   "metadata": {},
   "source": [
    "1. **Tokenização**\n",
    "    \n",
    "    Vamos dividir um texto em **palavras separadas (tokens)**, retirando pontuações e deixando tudo em minúsculas. Isso ajuda o sistema a identificar as partes do texto de forma organizada.\n",
    "    \n",
    "2. **Remoção de Stop Words**\n",
    "    \n",
    "    Em seguida, vamos remover palavras muito comuns como \"de\", \"o\", \"e\", que geralmente não têm valor para a análise de sentimento ou classificação.\n",
    "    \n",
    "3. **Stemming**\n",
    "    \n",
    "    Essa etapa reduz as palavras à sua **forma básica ou raiz**, por exemplo, transformando “amando”, “amou” e “amor” todos em algo como “am”. Isso evita que o sistema trate como palavras diferentes o que tem o mesmo sentido.\n",
    "    \n",
    "4. **Vetorização**\n",
    "    \n",
    "    Por fim, vamos converter as palavras em **números**. Isso é necessário porque os algoritmos de IA e aprendizado de máquina só entendem dados numéricos. Vamos criar uma estrutura chamada **\"Bag of Words\"**, que mostra quantas vezes cada palavra aparece no texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bddf5",
   "metadata": {},
   "source": [
    "## Etapa 1: Tokentização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56557b2d",
   "metadata": {},
   "source": [
    "Sem biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd17ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (sem biblioteca): ['o', 'filme', 'foi', 'muito', 'bom', 'adorei', 'a', 'atuação', 'e', 'o', 'final', 'surpreendente']\n"
     ]
    }
   ],
   "source": [
    "# Texto de exemplo\n",
    "texto = \"O filme foi muito bom, adorei a atuação e o final surpreendente!\"\n",
    "\n",
    "# Importa a lista de pontuação para ajudar a limpar o texto\n",
    "import string\n",
    "\n",
    "# Remove os sinais de pontuação do texto\n",
    "texto_limpo = ''.join([char for char in texto if char not in string.punctuation])\n",
    "\n",
    "# Converte o texto para minúsculas e separa por espaços\n",
    "tokens = texto_limpo.lower().split()\n",
    "\n",
    "# Mostra os tokens gerados\n",
    "print(\"Tokens (sem biblioteca):\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d0380",
   "metadata": {},
   "source": [
    "Com biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8d3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (com NLTK): ['o', 'filme', 'foi', 'muito', 'bom', ',', 'adorei', 'a', 'atuação', 'e', 'o', 'final', 'surpreendente', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download() pega uma memoria horrivel do pc\n",
    "\n",
    "# Usa a função word_tokenize para quebrar o texto em palavras\n",
    "tokens_nltk = word_tokenize(texto.lower())\n",
    "\n",
    "# Mostra os tokens\n",
    "print(\"Tokens (com NLTK):\", tokens_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cf6f4",
   "metadata": {},
   "source": [
    "## Etapa 2: Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e2769",
   "metadata": {},
   "source": [
    "Sem Biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8254472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words (sem biblioteca): ['filme', 'bom', 'adorei', 'atuação', 'final', 'surpreendente']\n"
     ]
    }
   ],
   "source": [
    "# Lista básica de stop words manuais\n",
    "stop_words = ['o', 'foi', 'a', 'e', 'de', 'do', 'da', 'os', 'as', 'um', 'uma', 'muito']\n",
    "\n",
    "# Remove as palavras da lista de stop words\n",
    "tokens_filtrados = [palavra for palavra in tokens if palavra not in stop_words]\n",
    "\n",
    "print(\"Tokens sem stop words (sem biblioteca):\", tokens_filtrados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce09b19",
   "metadata": {},
   "source": [
    "Com Biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7282cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words (com NLTK): ['filme', 'bom', ',', 'adorei', 'atuação', 'final', 'surpreendente', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pega a lista de stop words do português\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Filtra os tokens, removendo os que estão na lista de stop words\n",
    "tokens_filtrados_nltk = [palavra for palavra in tokens_nltk if palavra not in stopwords_pt]\n",
    "\n",
    "print(\"Tokens sem stop words (com NLTK):\", tokens_filtrados_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4ae5d",
   "metadata": {},
   "source": [
    "## Etapa 3: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806c355",
   "metadata": {},
   "source": [
    "Sem biblioteca (heurpistica simples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8033eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (sem biblioteca): ['filme', 'bom', 'ador', 'atua', 'final', 'surpreende']\n"
     ]
    }
   ],
   "source": [
    "# Função simples para remover sufixos comuns\n",
    "def stem(palavra):\n",
    "    sufixos = ['mente', 'ção', 'ções', 'nte', 'ndo', 'ado', 'ido', 'ados', 'idos', 'ei', 'ou']\n",
    "    for sufixo in sufixos:\n",
    "        if palavra.endswith(sufixo):\n",
    "            return palavra[:-len(sufixo)]\n",
    "    return palavra\n",
    "\n",
    "# Aplica o stemming em cada token\n",
    "tokens_stemmed = [stem(p) for p in tokens_filtrados]\n",
    "\n",
    "print(\"Stemming (sem biblioteca):\", tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30b072",
   "metadata": {},
   "source": [
    "Com biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63add886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (com NLTK): ['film', 'bom', ',', 'ador', 'atu', 'final', 'surpreend', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "# Inicializa o stemmer para português\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "# Aplica o stemming em cada token\n",
    "tokens_stemmed_nltk = [stemmer.stem(p) for p in tokens_filtrados_nltk]\n",
    "\n",
    "print(\"Stemming (com NLTK):\", tokens_stemmed_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed2550",
   "metadata": {},
   "source": [
    "## Etapa 4: Vetorização (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bae7c2",
   "metadata": {},
   "source": [
    "Sem biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e70888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['ador', 'atua', 'bom', 'filme', 'final', 'surpreende']\n",
      "Vetor (sem biblioteca): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Criar um vocabulário com palavras únicas\n",
    "vocabulario = sorted(list(set(tokens_stemmed)))\n",
    "\n",
    "# Cria o vetor com a frequência de cada palavra\n",
    "vetor = [tokens_stemmed.count(palavra) for palavra in vocabulario]\n",
    "\n",
    "# Exibe o vocabulário e o vetor\n",
    "print(\"Vocabulário:\", vocabulario)\n",
    "print(\"Vetor (sem biblioteca):\", vetor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f3b36",
   "metadata": {},
   "source": [
    "Com biblioteca (Scitik-learn):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4653fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário (com sklearn): ['adorei' 'atuação' 'bom' 'filme' 'final' 'foi' 'muito' 'surpreendente']\n",
      "Vetor (com sklearn): [1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus com o texto original para vetorização\n",
    "corpus = [\"O filme foi muito bom, adorei a atuação e o final surpreendente!\"]\n",
    "\n",
    "# Inicializa o vetorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Gera a matriz de frequência das palavras\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Exibe o vocabulário e a matriz\n",
    "print(\"Vocabulário (com sklearn):\", vectorizer.get_feature_names_out())\n",
    "print(\"Vetor (com sklearn):\", X.toarray()[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
