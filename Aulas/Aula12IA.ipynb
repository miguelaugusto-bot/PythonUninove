{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c4df2",
   "metadata": {},
   "source": [
    "# Aula 12 - Programação de Linguagem Natural - 31/10/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68605304",
   "metadata": {},
   "source": [
    "# O que é Programação de Linguagem Natural (PLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e044f81",
   "metadata": {},
   "source": [
    "**PLN (ou NLP - Natural Language Processing)** é uma subárea da Inteligência Artificial que permite aos computadores **entenderem, interpretarem e gerarem linguagem humana** de forma útil.\n",
    "\n",
    "Ela une **linguística computacional**, **estatística**, **aprendizado de máquina** e **ciência de dados** para extrair valor da linguagem humana.\n",
    "\n",
    "**Ex.:**\n",
    "1. **Chatbots (ex. atendimento automático no whatsapp ou sites)**\n",
    "- **Como é feito**: O chatbot utiliza PLN para entender as intenções do usuário. Ele identifica palavras-chave, faz análise sintática e semântica e aciona uma resposta.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização**: quebra a frase em palavras.\n",
    "    - **Reconhecimento de intenções (intent detection)**: identifica o que o usuário quer (ex: “quero pagar minha fatura” → intenção = pagamento).\n",
    "    - **Extração de entidades (NER)**: reconhece dados como datas, nomes, números.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"Quero saber meu saldo.\"*\n",
    "    - O chatbot detecta a intenção \"consultar_saldo\" e busca a informação no sistema.\n",
    "---\n",
    "2. **Corretores ortográficos**\n",
    "\n",
    "- **Como é feito**: O sistema compara as palavras escritas com um dicionário. Se uma palavra não está no vocabulário, o sistema sugere a mais parecida usando algoritmos como distância de Levenshtein.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização**: identifica cada palavra digitada.\n",
    "    - **Verificação de vocabulário**: confere se a palavra existe.\n",
    "    - **Sugerir correção**: calcula quais palavras são mais próximas.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"Recomendo esse filmee.\"*\n",
    "    - Saída: “Você quis dizer: *filme*?”\n",
    "---\n",
    "3. **Análise de sentimentos**\n",
    "\n",
    "- **Como é feito**: O sistema identifica se um texto expressa opinião positiva, negativa ou neutra.\n",
    "- **Etapas usadas**:\n",
    "    - **Remoção de stop words**, stemming.\n",
    "    - **Vetorização** e uso de modelos (estatísticos ou redes neurais).\n",
    "- **Exemplo real**:\n",
    "    - Comentário: *\"O filme foi incrível!\"*\n",
    "    - Saída: sentimento = *positivo*\n",
    "---\n",
    "4. **Tradução automática (Google Tradutor, DeepL)**\n",
    "\n",
    "- **Como é feito**: Utiliza redes neurais treinadas com milhões de pares de frases em dois idiomas.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização e análise gramatical**.\n",
    "    - **Mapeamento semântico**: entender o significado e a função de cada palavra.\n",
    "    - **Geração de texto no idioma de destino**.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"I love programming.\"*\n",
    "    - Tradução: *\"Eu amo programar.\"*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75139f00",
   "metadata": {},
   "source": [
    "# Etapas de Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161ffdc",
   "metadata": {},
   "source": [
    "1. **Tokenização**\n",
    "    \n",
    "    Vamos dividir um texto em **palavras separadas (tokens)**, retirando pontuações e deixando tudo em minúsculas. Isso ajuda o sistema a identificar as partes do texto de forma organizada.\n",
    "    \n",
    "2. **Remoção de Stop Words**\n",
    "    \n",
    "    Em seguida, vamos remover palavras muito comuns como \"de\", \"o\", \"e\", que geralmente não têm valor para a análise de sentimento ou classificação.\n",
    "    \n",
    "3. **Stemming**\n",
    "    \n",
    "    Essa etapa reduz as palavras à sua **forma básica ou raiz**, por exemplo, transformando “amando”, “amou” e “amor” todos em algo como “am”. Isso evita que o sistema trate como palavras diferentes o que tem o mesmo sentido.\n",
    "    \n",
    "4. **Vetorização**\n",
    "    \n",
    "    Por fim, vamos converter as palavras em **números**. Isso é necessário porque os algoritmos de IA e aprendizado de máquina só entendem dados numéricos. Vamos criar uma estrutura chamada **\"Bag of Words\"**, que mostra quantas vezes cada palavra aparece no texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bddf5",
   "metadata": {},
   "source": [
    "## Etapa 1: Tokentização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56557b2d",
   "metadata": {},
   "source": [
    "Sem biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd17ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (sem biblioteca): ['o', 'filme', 'foi', 'muito', 'bom', 'adorei', 'a', 'atuação', 'e', 'o', 'final', 'surpreendente']\n"
     ]
    }
   ],
   "source": [
    "# Texto de exemplo\n",
    "texto = \"O filme foi muito bom, adorei a atuação e o final surpreendente!\"\n",
    "\n",
    "# Importa a lista de pontuação para ajudar a limpar o texto\n",
    "import string\n",
    "\n",
    "# Remove os sinais de pontuação do texto\n",
    "texto_limpo = ''.join([char for char in texto if char not in string.punctuation])\n",
    "\n",
    "# Converte o texto para minúsculas e separa por espaços\n",
    "tokens = texto_limpo.lower().split()\n",
    "\n",
    "# Mostra os tokens gerados\n",
    "print(\"Tokens (sem biblioteca):\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d0380",
   "metadata": {},
   "source": [
    "Com biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8d3101",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/usr/local/python/3.12.1/nltk_data'\n    - '/usr/local/python/3.12.1/share/nltk_data'\n    - '/usr/local/python/3.12.1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Usa a função word_tokenize para quebrar o texto em palavras\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokens_nltk = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Mostra os tokens\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokens (com NLTK):\u001b[39m\u001b[33m\"\u001b[39m, tokens_nltk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/usr/local/python/3.12.1/nltk_data'\n    - '/usr/local/python/3.12.1/share/nltk_data'\n    - '/usr/local/python/3.12.1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Usa a função word_tokenize para quebrar o texto em palavras\n",
    "tokens_nltk = word_tokenize(texto.lower())\n",
    "\n",
    "# Mostra os tokens\n",
    "print(\"Tokens (com NLTK):\", tokens_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cf6f4",
   "metadata": {},
   "source": [
    "## Etapa 2: Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e2769",
   "metadata": {},
   "source": [
    "Sem Biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8254472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words (sem biblioteca): ['filme', 'bom', 'adorei', 'atuação', 'final', 'surpreendente']\n"
     ]
    }
   ],
   "source": [
    "# Lista básica de stop words manuais\n",
    "stop_words = ['o', 'foi', 'a', 'e', 'de', 'do', 'da', 'os', 'as', 'um', 'uma', 'muito']\n",
    "\n",
    "# Remove as palavras da lista de stop words\n",
    "tokens_filtrados = [palavra for palavra in tokens if palavra not in stop_words]\n",
    "\n",
    "print(\"Tokens sem stop words (sem biblioteca):\", tokens_filtrados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce09b19",
   "metadata": {},
   "source": [
    "Com Biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7282cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words (com NLTK): ['filme', 'bom', ',', 'adorei', 'atuação', 'final', 'surpreendente', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pega a lista de stop words do português\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Filtra os tokens, removendo os que estão na lista de stop words\n",
    "tokens_filtrados_nltk = [palavra for palavra in tokens_nltk if palavra not in stopwords_pt]\n",
    "\n",
    "print(\"Tokens sem stop words (com NLTK):\", tokens_filtrados_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4ae5d",
   "metadata": {},
   "source": [
    "## Etapa 3: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806c355",
   "metadata": {},
   "source": [
    "Sem biblioteca (heurpistica simples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8033eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (sem biblioteca): ['filme', 'bom', 'ador', 'atua', 'final', 'surpreende']\n"
     ]
    }
   ],
   "source": [
    "# Função simples para remover sufixos comuns\n",
    "def stem(palavra):\n",
    "    sufixos = ['mente', 'ção', 'ções', 'nte', 'ndo', 'ado', 'ido', 'ados', 'idos', 'ei', 'ou']\n",
    "    for sufixo in sufixos:\n",
    "        if palavra.endswith(sufixo):\n",
    "            return palavra[:-len(sufixo)]\n",
    "    return palavra\n",
    "\n",
    "# Aplica o stemming em cada token\n",
    "tokens_stemmed = [stem(p) for p in tokens_filtrados]\n",
    "\n",
    "print(\"Stemming (sem biblioteca):\", tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30b072",
   "metadata": {},
   "source": [
    "Com biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63add886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (com NLTK): ['film', 'bom', ',', 'ador', 'atu', 'final', 'surpreend', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "# Inicializa o stemmer para português\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "# Aplica o stemming em cada token\n",
    "tokens_stemmed_nltk = [stemmer.stem(p) for p in tokens_filtrados_nltk]\n",
    "\n",
    "print(\"Stemming (com NLTK):\", tokens_stemmed_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed2550",
   "metadata": {},
   "source": [
    "## Etapa 4: Vetorização (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bae7c2",
   "metadata": {},
   "source": [
    "Sem biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e70888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['ador', 'atua', 'bom', 'filme', 'final', 'surpreende']\n",
      "Vetor (sem biblioteca): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Criar um vocabulário com palavras únicas\n",
    "vocabulario = sorted(list(set(tokens_stemmed)))\n",
    "\n",
    "# Cria o vetor com a frequência de cada palavra\n",
    "vetor = [tokens_stemmed.count(palavra) for palavra in vocabulario]\n",
    "\n",
    "# Exibe o vocabulário e o vetor\n",
    "print(\"Vocabulário:\", vocabulario)\n",
    "print(\"Vetor (sem biblioteca):\", vetor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f3b36",
   "metadata": {},
   "source": [
    "Com biblioteca (Scitik-learn):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4653fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário (com sklearn): ['adorei' 'atuação' 'bom' 'filme' 'final' 'foi' 'muito' 'surpreendente'\n",
      " 'tudo']\n",
      "Vetor (com sklearn): [1 1 2 1 1 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus com o texto original para vetorização\n",
    "corpus = [\"O filme foi muito bom, adorei a atuação e o final surpreendente!, foi muito bom tudo\"]\n",
    "\n",
    "# Inicializa o vetorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Gera a matriz de frequência das palavras\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Exibe o vocabulário e a matriz\n",
    "print(\"Vocabulário (com sklearn):\", vectorizer.get_feature_names_out())\n",
    "print(\"Vetor (com sklearn):\", X.toarray()[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
