{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064c4df2",
   "metadata": {},
   "source": [
    "# Aula 12 - Programação de Linguagem Natural - 31/10/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68605304",
   "metadata": {},
   "source": [
    "# O que é Programação de Linguagem Natural (PLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e044f81",
   "metadata": {},
   "source": [
    "**PLN (ou NLP - Natural Language Processing)** é uma subárea da Inteligência Artificial que permite aos computadores **entenderem, interpretarem e gerarem linguagem humana** de forma útil.\n",
    "\n",
    "Ela une **linguística computacional**, **estatística**, **aprendizado de máquina** e **ciência de dados** para extrair valor da linguagem humana.\n",
    "\n",
    "**Ex.:**\n",
    "1. **Chatbots (ex. atendimento automático no whatsapp ou sites)**\n",
    "- **Como é feito**: O chatbot utiliza PLN para entender as intenções do usuário. Ele identifica palavras-chave, faz análise sintática e semântica e aciona uma resposta.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização**: quebra a frase em palavras.\n",
    "    - **Reconhecimento de intenções (intent detection)**: identifica o que o usuário quer (ex: “quero pagar minha fatura” → intenção = pagamento).\n",
    "    - **Extração de entidades (NER)**: reconhece dados como datas, nomes, números.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"Quero saber meu saldo.\"*\n",
    "    - O chatbot detecta a intenção \"consultar_saldo\" e busca a informação no sistema.\n",
    "---\n",
    "2. **Corretores ortográficos**\n",
    "\n",
    "- **Como é feito**: O sistema compara as palavras escritas com um dicionário. Se uma palavra não está no vocabulário, o sistema sugere a mais parecida usando algoritmos como distância de Levenshtein.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização**: identifica cada palavra digitada.\n",
    "    - **Verificação de vocabulário**: confere se a palavra existe.\n",
    "    - **Sugerir correção**: calcula quais palavras são mais próximas.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"Recomendo esse filmee.\"*\n",
    "    - Saída: “Você quis dizer: *filme*?”\n",
    "---\n",
    "3. **Análise de sentimentos**\n",
    "\n",
    "- **Como é feito**: O sistema identifica se um texto expressa opinião positiva, negativa ou neutra.\n",
    "- **Etapas usadas**:\n",
    "    - **Remoção de stop words**, stemming.\n",
    "    - **Vetorização** e uso de modelos (estatísticos ou redes neurais).\n",
    "- **Exemplo real**:\n",
    "    - Comentário: *\"O filme foi incrível!\"*\n",
    "    - Saída: sentimento = *positivo*\n",
    "---\n",
    "4. **Tradução automática (Google Tradutor, DeepL)**\n",
    "\n",
    "- **Como é feito**: Utiliza redes neurais treinadas com milhões de pares de frases em dois idiomas.\n",
    "- **Etapas usadas**:\n",
    "    - **Tokenização e análise gramatical**.\n",
    "    - **Mapeamento semântico**: entender o significado e a função de cada palavra.\n",
    "    - **Geração de texto no idioma de destino**.\n",
    "- **Exemplo real**:\n",
    "    - Entrada: *\"I love programming.\"*\n",
    "    - Tradução: *\"Eu amo programar.\"*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75139f00",
   "metadata": {},
   "source": [
    "# Etapas de Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161ffdc",
   "metadata": {},
   "source": [
    "1. **Tokenização**\n",
    "    \n",
    "    Vamos dividir um texto em **palavras separadas (tokens)**, retirando pontuações e deixando tudo em minúsculas. Isso ajuda o sistema a identificar as partes do texto de forma organizada.\n",
    "    \n",
    "2. **Remoção de Stop Words**\n",
    "    \n",
    "    Em seguida, vamos remover palavras muito comuns como \"de\", \"o\", \"e\", que geralmente não têm valor para a análise de sentimento ou classificação.\n",
    "    \n",
    "3. **Stemming**\n",
    "    \n",
    "    Essa etapa reduz as palavras à sua **forma básica ou raiz**, por exemplo, transformando “amando”, “amou” e “amor” todos em algo como “am”. Isso evita que o sistema trate como palavras diferentes o que tem o mesmo sentido.\n",
    "    \n",
    "4. **Vetorização**\n",
    "    \n",
    "    Por fim, vamos converter as palavras em **números**. Isso é necessário porque os algoritmos de IA e aprendizado de máquina só entendem dados numéricos. Vamos criar uma estrutura chamada **\"Bag of Words\"**, que mostra quantas vezes cada palavra aparece no texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bddf5",
   "metadata": {},
   "source": [
    "## Etapa 1: Tokentização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56557b2d",
   "metadata": {},
   "source": [
    "Sem biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd17ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (sem biblioteca): ['o', 'filme', 'foi', 'muito', 'bom', 'adorei', 'a', 'atuação', 'e', 'o', 'final', 'surpreendente']\n"
     ]
    }
   ],
   "source": [
    "# Texto de exemplo\n",
    "texto = \"O filme foi muito bom, adorei a atuação e o final surpreendente!\"\n",
    "\n",
    "# Importa a lista de pontuação para ajudar a limpar o texto\n",
    "import string\n",
    "\n",
    "# Remove os sinais de pontuação do texto\n",
    "texto_limpo = ''.join([char for char in texto if char not in string.punctuation])\n",
    "\n",
    "# Converte o texto para minúsculas e separa por espaços\n",
    "tokens = texto_limpo.lower().split()\n",
    "\n",
    "# Mostra os tokens gerados\n",
    "print(\"Tokens (sem biblioteca):\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d0380",
   "metadata": {},
   "source": [
    "Com biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8d3101",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/usr/local/python/3.12.1/nltk_data'\n    - '/usr/local/python/3.12.1/share/nltk_data'\n    - '/usr/local/python/3.12.1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Usa a função word_tokenize para quebrar o texto em palavras\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokens_nltk = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Mostra os tokens\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokens (com NLTK):\u001b[39m\u001b[33m\"\u001b[39m, tokens_nltk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/usr/local/python/3.12.1/nltk_data'\n    - '/usr/local/python/3.12.1/share/nltk_data'\n    - '/usr/local/python/3.12.1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Usa a função word_tokenize para quebrar o texto em palavras\n",
    "tokens_nltk = word_tokenize(texto.lower())\n",
    "\n",
    "# Mostra os tokens\n",
    "print(\"Tokens (com NLTK):\", tokens_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cf6f4",
   "metadata": {},
   "source": [
    "## Etapa 2: Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e2769",
   "metadata": {},
   "source": [
    "Sem Biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8254472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words (sem biblioteca): ['filme', 'bom', 'adorei', 'atuação', 'final', 'surpreendente']\n"
     ]
    }
   ],
   "source": [
    "# Lista básica de stop words manuais\n",
    "stop_words = ['o', 'foi', 'a', 'e', 'de', 'do', 'da', 'os', 'as', 'um', 'uma', 'muito']\n",
    "\n",
    "# Remove as palavras da lista de stop words\n",
    "tokens_filtrados = [palavra for palavra in tokens if palavra not in stop_words]\n",
    "\n",
    "print(\"Tokens sem stop words (sem biblioteca):\", tokens_filtrados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce09b19",
   "metadata": {},
   "source": [
    "Com Biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7282cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sem stop words (com NLTK): ['filme', 'bom', ',', 'adorei', 'atuação', 'final', 'surpreendente', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pega a lista de stop words do português\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Filtra os tokens, removendo os que estão na lista de stop words\n",
    "tokens_filtrados_nltk = [palavra for palavra in tokens_nltk if palavra not in stopwords_pt]\n",
    "\n",
    "print(\"Tokens sem stop words (com NLTK):\", tokens_filtrados_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4ae5d",
   "metadata": {},
   "source": [
    "## Etapa 3: Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806c355",
   "metadata": {},
   "source": [
    "Sem biblioteca (heurpistica simples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8033eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (sem biblioteca): ['filme', 'bom', 'ador', 'atua', 'final', 'surpreende']\n"
     ]
    }
   ],
   "source": [
    "# Função simples para remover sufixos comuns\n",
    "def stem(palavra):\n",
    "    sufixos = ['mente', 'ção', 'ções', 'nte', 'ndo', 'ado', 'ido', 'ados', 'idos', 'ei', 'ou']\n",
    "    for sufixo in sufixos:\n",
    "        if palavra.endswith(sufixo):\n",
    "            return palavra[:-len(sufixo)]\n",
    "    return palavra\n",
    "\n",
    "# Aplica o stemming em cada token\n",
    "tokens_stemmed = [stem(p) for p in tokens_filtrados]\n",
    "\n",
    "print(\"Stemming (sem biblioteca):\", tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30b072",
   "metadata": {},
   "source": [
    "Com biblioteca (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63add886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (com NLTK): ['film', 'bom', ',', 'ador', 'atu', 'final', 'surpreend', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "# Inicializa o stemmer para português\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "# Aplica o stemming em cada token\n",
    "tokens_stemmed_nltk = [stemmer.stem(p) for p in tokens_filtrados_nltk]\n",
    "\n",
    "print(\"Stemming (com NLTK):\", tokens_stemmed_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed2550",
   "metadata": {},
   "source": [
    "## Etapa 4: Vetorização (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bae7c2",
   "metadata": {},
   "source": [
    "Sem biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e70888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['ador', 'atua', 'bom', 'filme', 'final', 'surpreende']\n",
      "Vetor (sem biblioteca): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Criar um vocabulário com palavras únicas\n",
    "vocabulario = sorted(list(set(tokens_stemmed)))\n",
    "\n",
    "# Cria o vetor com a frequência de cada palavra\n",
    "vetor = [tokens_stemmed.count(palavra) for palavra in vocabulario]\n",
    "\n",
    "# Exibe o vocabulário e o vetor\n",
    "print(\"Vocabulário:\", vocabulario)\n",
    "print(\"Vetor (sem biblioteca):\", vetor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f3b36",
   "metadata": {},
   "source": [
    "Com biblioteca (Scitik-learn):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus com o texto original para vetorização\n",
    "corpus = [\"O filme foi muito bom, adorei a atuação e o final surpreendente!, foi muito bom tudo\"]\n",
    "\n",
    "# Inicializa o vetorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Gera a matriz de frequência das palavras\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Exibe o vocabulário e a matriz\n",
    "print(\"Vocabulário (com sklearn):\", vectorizer.get_feature_names_out())\n",
    "print(\"Vetor (com sklearn):\", X.toarray()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380f4e7",
   "metadata": {},
   "source": [
    "## Resumindo as etapas de Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a38906",
   "metadata": {},
   "source": [
    "| Etapa | Sem Biblioteca | Com Biblioteca |\n",
    "|-------------|--------------|--------------|\n",
    "| Tokenização  | split() + string.punctuation   | nltk.tokenize.word_tokenize()   |\n",
    "| Stop Words  | lista manual + filtro   | nltk.corpus.stopwords.words()   |\n",
    "| Stemming  | função que corta sufixos   | nltk.stem.RSLPStemmer()   |\n",
    "| Vetorização  | set(), count()   | sklearn.feature_extraction.CountVectorizer   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7518324",
   "metadata": {},
   "source": [
    "# O que é Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f68e6",
   "metadata": {},
   "source": [
    "O Naive Bayes é um algoritmo de classificação probabilística baseado no Teorema de Bayes. Ele é chamado de \"naive\" (ingênuo) porque assume que todas as palavras do texto são independentes entre si, o que nem sempre é verdade — mas essa simplificação funciona muito bem na prática, principalmente para PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22a7a9",
   "metadata": {},
   "source": [
    "## Fórmula do Teorema de Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cdf920",
   "metadata": {},
   "source": [
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Na classificação de texto, ela é adaptada para:\n",
    "\n",
    "$$P(classe|documento) \\propto P(classe) \\cdot \\prod_{i=1}^{n} P(palavra_i|classe)$$\n",
    "\n",
    "**Explicando os termos:**\n",
    "- **P(classe∣documento)**: Probabilidade do documento pertencer a uma classe (ex: *bom* ou *ruim*).\n",
    "- **P(classe)**: Probabilidade da classe ocorrer (ex: frequência de comentários positivos no treino).\n",
    "- **P(palavrai∣classe)**: Probabilidade da palavra aparecer em textos daquela classe.\n",
    "- **∏**: Produto das probabilidades de todas as palavras do documento (assumindo independência).\n",
    "\n",
    "---\n",
    "\n",
    "**Como ele funciona ? (parte prática)**\n",
    "**Exemplo:**\n",
    "Vamos dizer que queremos classificar o comentário:\n",
    "\n",
    "\"filme ótimo\"\n",
    "\n",
    "\n",
    "Nos dados de treino temos:\n",
    "- 3 exemplos da classe **bom**\n",
    "- 3 exemplos da classe **ruim**\n",
    "\n",
    "---\n",
    "\n",
    "**Passo a passo do algoritmo:**\n",
    "1. **Calcula P(bom) e P(ruim)**\n",
    "    - P(bom)=3/6=0.5\n",
    "    - P(ruim)=3/6=0.5\n",
    "2. **Calcula P(filme∣bom), P(ótimo|bom)**\n",
    "    - Contamos quantas vezes \"filme\" aparece nos comentários bons, dividimos pelo total de palavras em \"bom\"\n",
    "    - Se não aparece, usamos **suavização de Laplace** para evitar zero.\n",
    "3. **Faz o mesmo para a classe ruim**\n",
    "4. **Multiplica tudo** para cada classe:\n",
    "$$P(bom∣comentario)∝P(bom)⋅P(filme∣bom)⋅P(ótimo∣bom)$$\n",
    "$$P(ruim∣comentario)∝P(ruim)⋅P(filme∣ruim)⋅P(ótimo∣ruim)$$\n",
    "\n",
    "Escolhe a maior probabilidade\n",
    "\n",
    "---\n",
    "\n",
    "**O que é Suavização de Laplace?**  \n",
    "Quando uma palavra **não aparece** nos dados de uma classe, a probabilidade **P(palavra∣classe)** vira **zero** — o que anula toda a multiplicação.\n",
    "\n",
    "Para evitar isso, usamos Laplace:\n",
    "$$P(palavra|classe) = \\frac{contagem(palavra) + 1}{total\\_palavras + vocabulário\\_tamanho}$$\n",
    "\n",
    "---\n",
    "\n",
    "**Vantagens do Naive Bayes:**  \n",
    "- Rápido e eficiente para grandes volumes de texto.\n",
    "- Funciona bem mesmo com dados simples.\n",
    "- Fácil de entender e explicar (ótimo para iniciar em PLN).\n",
    "\n",
    "\"O Naive Bayes aprende com exemplos, analisa cada palavra do comentário novo, calcula a chance dele ser positivo ou negativo com base nas palavras que aprendeu, e decide qual é a mais provável.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1624c30",
   "metadata": {},
   "source": [
    "# Implementação da classificação de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c535a1",
   "metadata": {},
   "source": [
    "## 1 - Implementação sem biblioteca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61932540",
   "metadata": {},
   "source": [
    "### 1. Base de treino (comentários e sentimentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f726e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino = [\n",
    "    (\"gostei do filme\", \"bom\"),\n",
    "    (\"não gostei do filme\", \"ruim\"),\n",
    "    (\"ótimo filme\", \"bom\"),\n",
    "    (\"péssimo e longo\", \"ruim\"),\n",
    "    (\"amei a história\", \"bom\"),\n",
    "    (\"foi horrível\", \"ruim\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e7656",
   "metadata": {},
   "source": [
    "### 2. Base de Teste (sem sentimento conhecido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a725037",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_teste = [\n",
    "    \"o filme foi ótimo\",\n",
    "    \"não gostei do final\",\n",
    "    \"adorei a trilha sonora\",\n",
    "    \"filme muito ruim\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c629b",
   "metadata": {},
   "source": [
    "### 3. Classificador manual (baseado em palavras positivas/negativas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bc6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras associadas a sentimentos\n",
    "palavras_bom = [\"gostei\", \"ótimo\", \"amei\", \"adorei\", \"bom\", \"excelente\"]\n",
    "palavras_ruim = [\"não\", \"péssimo\", \"horrível\", \"ruim\", \"detestei\"]\n",
    "\n",
    "def classificar(comentario):\n",
    "    tokens = comentario.lower().split()\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in palavras_bom:\n",
    "            score += 1\n",
    "        elif token in palavras_ruim:\n",
    "            score -= 1\n",
    "    if score > 0:\n",
    "        return \"bom\"\n",
    "    elif score < 0:\n",
    "        return \"ruim\"\n",
    "    else:\n",
    "        return \"neutro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d47657",
   "metadata": {},
   "source": [
    "### 4. Aplicar classificado e calcular porcentagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac21f25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'o filme foi ótimo' => bom\n",
      "'não gostei do final' => neutro\n",
      "'adorei a trilha sonora' => bom\n",
      "'filme muito ruim' => ruim\n",
      "\n",
      "Resumo de Sentimentos:\n",
      "Bom: 50.00%\n",
      "Neutro: 25.00%\n",
      "Ruim: 25.00%\n"
     ]
    }
   ],
   "source": [
    "# Classifica cada comentário\n",
    "resultados = [(comentario, classificar(comentario)) for comentario in comentarios_teste]\n",
    "\n",
    "# Exibe os resultados\n",
    "for c, r in resultados:\n",
    "    print(f\"'{c}' => {r}\")\n",
    "\n",
    "# Calcula porcentagem\n",
    "from collections import Counter\n",
    "\n",
    "contagem = Counter([r for _, r in resultados])\n",
    "total = sum(contagem.values())\n",
    "\n",
    "print(\"\\nResumo de Sentimentos:\")\n",
    "for sentimento, qtd in contagem.items():\n",
    "    print(f\"{sentimento.capitalize()}: {qtd / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2775871",
   "metadata": {},
   "source": [
    "## 2. Com biblioteca (Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517d103",
   "metadata": {},
   "source": [
    "### 1. Base de Treinamento (comentários e sentimentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63a94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comentários de treino e seus respectivos sentimentos\n",
    "comentarios_treino = [\n",
    "    \"gostei do filme\",\n",
    "    \"não gostei do filme\",\n",
    "    \"ótimo filme\",\n",
    "    \"péssimo e longo\",\n",
    "    \"amei a história\",\n",
    "    \"foi horrível\"\n",
    "]\n",
    "\n",
    "sentimentos_treino = [\n",
    "    \"bom\",\n",
    "    \"ruim\",\n",
    "    \"bom\",\n",
    "    \"ruim\",\n",
    "    \"bom\",\n",
    "    \"ruim\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228d02b",
   "metadata": {},
   "source": [
    "### 2. Base de Teste (sem sentimento conhecido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f140a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_teste = [\n",
    "    \"o filme foi ótimo\",\n",
    "    \"não gostei do final\",\n",
    "    \"adorei a trilha sonora\",\n",
    "    \"filme muito ruim\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f9a74",
   "metadata": {},
   "source": [
    "### 3. Modelo com CountVectorizer + MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e64b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o filme foi ótimo => bom\n",
      "não gostei do final => ruim\n",
      "adorei a trilha sonora => bom\n",
      "filme muito ruim => bom\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Cria o pipeline: vetorização das palavras + classificador Naive Bayes\n",
    "modelo = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Treina o modelo com os dados de treino\n",
    "modelo.fit(comentarios_treino, sentimentos_treino)\n",
    "\n",
    "# Aplica o modelo nos comentários de teste\n",
    "previsoes = modelo.predict(comentarios_teste)\n",
    "\n",
    "# Exibe os resultados\n",
    "for comentario, sentimento in zip(comentarios_teste, previsoes):\n",
    "    print(f\"{comentario} => {sentimento}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd81d8e",
   "metadata": {},
   "source": [
    "### 4. Porcentagem dos sentimentos classificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef75527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumo de Sentimentos:\n",
      "Bom: 75.00%\n",
      "Ruim: 25.00%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Conta quantos comentários foram classificados em cada categoria\n",
    "contagem = Counter(previsoes)\n",
    "total = sum(contagem.values())\n",
    "\n",
    "# Calcula e mostra a porcentagem\n",
    "print(\"\\nResumo de Sentimentos:\")\n",
    "for sentimento, qtd in contagem.items():\n",
    "    percentual = (qtd / total) * 100\n",
    "    print(f\"{sentimento.capitalize()}: {percentual:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718aa9ff",
   "metadata": {},
   "source": [
    "## Observações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6d1ef",
   "metadata": {},
   "source": [
    "- **`CountVectorizer()`** transforma o texto em vetores numéricos (bag-of-words).\n",
    "- **`MultinomialNB()`** é um modelo de classificação adequado para dados de frequência (como palavras).\n",
    "- A **pipeline** garante que os dados passem pelas etapas certas na ordem correta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
